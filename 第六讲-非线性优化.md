# 第六讲:非线性优化

## 状态估计问题

### 最大后验与最大似然

第二讲中介绍过经典的slam模型由一个运动方程和一个观测方程构成:
$$
\begin{cases}
	x_k = f(x_{k-1},u_k,w_k)+w_k \qquad w_k 为运动方程噪声\\
	z_{k,j}=h(y_j,x_k,v_{k,j})-v_{k,j} \qquad v_{k,j}为观测方程噪声
\end{cases}
$$

通常假设两个噪声满足均匀高斯分布:
$$
w_k \sim  N(0,R_k), \quad v_k \sim N(0,Q_{k,j})
$$

从概率学的角度看,在非线性优化中把所有的待估计变量放在一个状态变量中:
$$
x=\{x_1,\cdots,x_N,y_1,\cdots,y_M\}
$$

现在对机器人的状态估计,就是已知输入数据$u$,和观测数据$z$的条件下, 求计算机状态$x$的条件概率分布:
$$
P(x|z,u)
$$

当没有测量数据,即只考虑z时,相当于估计$P(x|z)$的条件概率分布. 

如果忽略图像在时间上的联系,把他们看着一堆彼此没有关系的图片,该问题称为`Sructure from Motion`(sfm),即如何从许多图像中重建三维结构.

SLAM可以看做是图像具有时间先后顺序,需要实时求解一个sfm问题.

为了估计状态变量的条件分布,利用贝叶斯公式:
$$
P(x|z) = \frac{P(z|x)P(x)}{P(z)}
$$

- P(x|z) 为`后验概率`
- P(z|x) 为`似然`
- P(x) 为`先验`

>直接求后验分布是困难的, 但是求一个状态最优估计,使得该状态下后验概率最大化,则可进行:
$$
x^*_{MAP} = \arg\max P(x|z) = \argmax P(z|x)P(x)
$$
$x^*=\argmax f(x)$表示当x=x^*时,f(x)取得最大值.

求解最大后验概率 **相当于最大化`似然`和`先验`的乘积**

如果不知道机器人的位姿大概在什么位置,可以求解x的最大释然估计:
$$
x^*_{MAP} = \argmax P(z|x)
$$

>**似然**: 指在现在的位姿下可能产生怎样的观测数据.
**最大似然估计**: 在什么样的状态下最可能产生观测到的数据.


### 最小二乘的引出

对于一次观测:
$$
z_{k,j} = h(y_j,x_k) +v_{k,j}
$$

由于假设了噪声项$u_k \sim N(0,Q_{k,j})$,所以观测数据的条件概率为:
$$
P(z_{j,k}|x_k,y_j) = N(h(y_j,x_k),Q_{k,j})
$$

上式依然是一个高斯分布,为了计算使它最大化的$想x_k(位置),y_i(路标)$,使用 **最小负对数**来求高斯分布的最大似然.

高斯分布$x \sim N(\mu, \sum)$,它的概率密度展开形式为:
$$
P(x) = \frac{1}{\sqrt {(2\pi)^N \det(\sum) }} \exp(-\frac{(x- \mu)^T}{2\sum{(x-\mu)}})
$$

对其取负对数

$$
-\ln(P(x)) = \frac{ln((2\pi)^N\det(\sum))}{2} + \frac{(x- \mu)^T}{2\sum{(x-\mu)}}
$$

> $\sum$ 表示协方差矩阵
> $\det$ 表示求行列式的值

对原分布求最大化相当于对负对数求最小化. $\cdots$

定义`数据`和`估计值`之间的误差.
$$
e_{v,k} = x_k - f(x_{k-1},u_k) \newline
e_{y,j,k} = z_{k,j} -h(x_k,y_j)
$$

求误差平方和:
$$
J(x) = \sum_k {e_{v,k}^TR_k^{-1}e_{v,k}} + \sum_k\sum_j{ e^T_{y,k,j}Q^{-1}_{k.j}e_{y,k,j} }
$$

这样得到了总体意义下的最小二乘问题. 它的最优解等价于状态的最大似然估计. 

由于噪声的存在,我们把估计的轨迹带入SLAM的运动,观测方程时,他们并不会完美成立. 我们对状态估计进行`微调`,使得整体误差下降一些.一般会得到一个`极小值`. 这就是一个典型的`非线性优化`过程.

### 非线性最小二乘法

一个简单的最小二乘问题:
$$
\min \frac{1}{2}||f(x)||_2^2
$$

对x求导可以得到极值:
$$
 \frac{d(\frac{1}{2}||f(x)||^2_2)}{dx} =0
$$

如果$f(x)$比较复杂,那么用求导的方法是比较困难的.

对于不方便直接求解的最小二乘问题,可以用迭代的方式:
1. 给定某个初始值$x_0$
2. 对于第$k$次迭代,寻找一个增量$\Delta x_k$, 使得$||f(x_k+\Delta x_k)||^2_2$达到最小值
3. 若$\Delta x_k$ 足够小,则停止.
4. 否则令$x_{k+1} = x_k +\Delta x_k$,返回第二步


关键问题是增量$\Delta x_k$ 如何确定.

在slam中有两类方法.下面将介绍这两种方法.

### 一阶和二阶梯度法

将目标函数在$x$附近进行泰勒展开:
$$
\|f(x+\Delta x)\|^2_2 \approx \|f(x)\|^2_2 + J(x) \Delta x + \frac{1}{2} \Delta x^TH \Delta x
$$

>$J$ 是$\|f(x)\|^2_2$关于x的导数, 而H则是二阶导数.

如果保留一阶梯度,那么增量的解为:
$$
\Delta x^* = -J^T(x)
$$

如果保留二阶梯度信息,增量方程为:
$$
\Delta x^* = \argmin \|f(x)\|^2_2 + J(x)\Delta x + \frac{1}{2}\Delta x^TH \Delta x
$$

求右侧关于$\Delta x$的导数并令它为0, 就得到了增量的解:
$$
H \Delta x = -J^T
$$

>该方法又称为牛顿法. 牛顿法需要计算目标函数的H矩阵,我们通常避免H的计算,接下来的方法即解决改问题.

### 牛顿高斯法

思想: 将$非(x)$进行泰勒展开.
$$
f(x +\Delta x) \approx f(x) +J(x) \Delta x
$$
>$J(x)$是$f(x)$关于x的导数.

当前目前:寻找下降矢量:$\Delta x$, 使得$\|f(x+ \Delta x)\|^2_2$达到最小. 
$$
\Delta x^* = \argmin \frac{1}{2}\|f(x)+J(x)\Delta x\|^2
$$

展开平方项并对$\Delta x$ 求导,得:
$$
J(x)^TJ(x)\Delta x = - J(x)^Tf(x)
$$

这是一个线性方程, 被称为:`增量方程`或`高斯牛顿方程`或`正规方程`.

定义:

- $J(x)^TJ(x)$ 为 $H$
- J(x)^Tf(x) 为 $g$

上式可记为:
$$
H \Delta x = g
$$

>求解增量方程是整个优化过程的核心所在.

高斯牛顿法的步骤:

1. 给定初始值$x_0$
2. 对于第k次迭代, 求出当前雅可比矩阵$J(x_k)$和误差$f(x_k)$
3. 求解增量方程:$H \Delta x_k = g$
4. 若$\Delta x_k$足够小, 则停止. 否则,令$x_{k+1} = x_k +\Delta x_k$,返回第二步


高斯牛顿法的缺点:

- 可能出现$J^TJ$为奇异矩阵或者病态
- 如果求出的$\Delta x$过大,也会导致算法不稳定.

在非线性优化里,相当多的方法都是高斯牛顿法的变种.


### 列文伯格-马夸尔特方法

高是牛顿法中采用的近似二阶泰勒展开只能在展开点附近有较好的效果, 所以我们可以给$\Delta x$一个信赖区域.这种方法被称为`信赖区域法`.

如何确定信赖区域的范围?

根据我们的近似模型跟实际函数之间的差异来确定: 如果差异小,扩大范围,如果差异大,缩小范围.

因此使用$\rho$来判断泰勒近似是否好:
$$ 
\rho = \frac{f(x+\Delta x)-f(x)}{J(x)\Delta x}
$$

$\rho$接近于1 说明近似效果好. 

如果$\rho$太小,说明近似差,需要缩小范围, 反之放大范围.

于是有了一个改良版的非线性优化框架:

1. 给定初始值$x_0$, 以及初始优化半径$\mu$
2. 对于第k次迭代,求解:
	$$
	\min \limits_{\Delta x_k} \frac{1}{2}\|f(x_k) + J(x_k)\Delta x_k \|^2 , \quad s.t. \quad \|D \Delta x_k \|^2 \leqslant \mu
	$$
	- $\mu$ 是信赖区域半径
	- D 将会在后面说明
3. 计算$\rho$
4. 若 $\rho > \frac{3}{4}$, 则$\mu = 2 \mu$
5. 若$\rho < \frac{1}{4}$, 则$\mu = 0.5 \mu$
6. 如果$\rho$大于阈值,则认为近似可行. 令$x_{k+1}= x_k+\Delta x_k$
7. 判断算法是都收敛.如果不收敛返回第二步,否则结束.

